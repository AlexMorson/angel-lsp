import {TokenKind, TokenObject, TokenString} from "../compiler_tokenizer/tokenObject";
import {diagnostic} from "../code/diagnostic";
import {HighlightForToken} from "../code/highlight";
import {TextLocation} from "../compiler_tokenizer/textLocation";

/**
 * Output of the 'preprocessTokensForParser' function.
 */
export interface PreprocessedTokenOutput {
    readonly parsingTokens: TokenObject[];
    readonly includeFiles: TokenObject[];
}

/**
 * Preprocess the token list for the parser.
 * For example, remove comments, concatenate continuous strings, and process directives.
 * @param tokens Tokens generated by the tokenizer.
 */
export function preprocessTokensForParser(tokens: TokenObject[]): PreprocessedTokenOutput {
    // Remove comments
    const actualTokens: TokenObject[] = tokens.filter(t => t.kind !== TokenKind.Comment);

    // Handle preprocessor directives
    const includeFiles = preprocessDirectives(actualTokens);

    // Concatenate continuous strings.
    for (let i = actualTokens.length - 1; i >= 1; i--) {
        const isContinuousString = actualTokens[i].kind === TokenKind.String && actualTokens[i - 1].kind === TokenKind.String;
        if (isContinuousString === false) continue;

        // Create a new token with the combined elements.
        actualTokens[i - 1] = createConnectedStringTokenAt(actualTokens, i);
        actualTokens.splice(i, 1);
    }

    // Assign index information.
    for (let i = 0; i < actualTokens.length; i++) {
        actualTokens[i].setPreprocessedTokenInfo(i, i != actualTokens.length - 1 ? actualTokens[i + 1] : undefined);
    }

    return {
        parsingTokens: actualTokens,
        includeFiles: includeFiles
    };
}

function preprocessDirectives(tokens: TokenObject[]): TokenObject[] {
    const includeFiles: TokenObject[] = [];
    const directiveRanges: [number, number][] = [];

    // Handle preprocessor directives starting with '#'
    for (let i = 0; i < tokens.length; i++) {
        if (tokens[i].text !== '#') continue;
        const directiveTokens = sliceTokenListBySameLine(tokens, i);

        handleDirectiveTokens(directiveTokens, includeFiles);
        directiveRanges.push([i, directiveTokens.length]);
    }

    // Remove preprocessor directives.
    for (let i = directiveRanges.length - 1; i >= 0; i--) {
        tokens.splice(directiveRanges[i][0], directiveRanges[i][1]);
    }

    return includeFiles;
}

function handleDirectiveTokens(directiveTokens: TokenObject[], includeFiles: TokenObject[]) {
    directiveTokens[0].setHighlight(HighlightForToken.Directive);

    if (directiveTokens[1]?.text === 'include') {
        directiveTokens[1].setHighlight(HighlightForToken.Directive);

        // Check the include directive.
        const fileName = directiveTokens[2];
        if (fileName === undefined) {
            diagnostic.addError(directiveTokens[1].location, 'Expected file name for include directive.');
            return;
        }

        if (fileName.kind !== TokenKind.String) {
            diagnostic.addError(directiveTokens[2].location, 'Expected string literal for include directive.');
            return;
        }

        includeFiles.push(fileName);
    } else {
        if (directiveTokens[1] != null) directiveTokens[1].setHighlight(HighlightForToken.Label);
    }
}

function sliceTokenListBySameLine(tokens: TokenObject[], head: number): TokenObject[] {
    let tail = head;
    for (let i = head; i < tokens.length - 1; i++) {
        if (tokens[i].location.end.isSameLine(tokens[i + 1].location.start) === false) {
            break;
        }

        tail = i + 1;
    }

    return tokens.slice(head, tail + 1);
}

function createConnectedStringTokenAt(actualTokens: TokenObject[], index: number): TokenObject {
    const tokenText = actualTokens[index].text + actualTokens[index + 1].text;

    const tokenLocation = new TextLocation(
        actualTokens[index].location.path,
        actualTokens[index].location.start,
        actualTokens[index + 1].location.end
    );

    return TokenString.createVirtual(tokenText, tokenLocation);
}

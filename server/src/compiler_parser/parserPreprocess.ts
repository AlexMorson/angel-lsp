import {TokenizerToken, TokenKind} from "../compiler_tokenizer/tokens";
import {ParserToken} from "./parserToken";
import {diagnostic} from "../code/diagnostic";
import {HighlightToken} from "../code/highlight";
import {Mutable} from "../utils/utilities";
import {createVirtualToken, isSameLine} from "../compiler_tokenizer/tokenUtils";

/**
 * Output of the 'preprocessTokensForParser' function.
 */
export interface PreprocessedTokenOutput {
    readonly parsingTokens: ParserToken[];
    readonly includeFiles: TokenizerToken[];
}

/**
 * Preprocess the token list for the parser.
 * For example, remove comments, concatenate continuous strings, and process directives.
 * @param tokens Tokens generated by the tokenizer.
 */
export function preprocessTokensForParser(tokens: TokenizerToken[]): PreprocessedTokenOutput {
    // Remove comments
    const actualTokens: Mutable<ParserToken>[] = tokens.filter(t => t.kind !== TokenKind.Comment).map(token => {
        return {
            ...token,
            index: -1,
            next: undefined
        };
    });

    // Handle preprocessor directives
    const includeFiles = preprocessDirectives(actualTokens);

    // Concatenate continuous strings.
    for (let i = actualTokens.length - 1; i >= 1; i--) {
        const isContinuousString = actualTokens[i].kind === TokenKind.String && actualTokens[i - 1].kind === TokenKind.String;
        if (isContinuousString === false) continue;

        // Create a new token with the combined elements.
        actualTokens[i - 1] = createConnectedStringTokenAt(actualTokens, i);
        actualTokens.splice(i, 1);
    }

    // Assign index information.
    for (let i = 0; i < actualTokens.length; i++) {
        actualTokens[i].index = i;
        actualTokens[i].next = i != actualTokens.length - 1 ? actualTokens[i + 1] : undefined;
    }

    return {
        parsingTokens: actualTokens,
        includeFiles: includeFiles
    };
}

function preprocessDirectives(tokens: TokenizerToken[]): TokenizerToken[] {
    const includeFiles: TokenizerToken[] = [];
    const directiveRanges: [number, number][] = [];

    // Handle preprocessor directives starting with '#'
    for (let i = 0; i < tokens.length; i++) {
        if (tokens[i].text !== '#') continue;
        const directiveTokens = sliceTokenListBySameLine(tokens, i);

        handleDirectiveTokens(directiveTokens, includeFiles);
        directiveRanges.push([i, directiveTokens.length]);
    }

    // Remove preprocessor directives.
    for (let i = directiveRanges.length - 1; i >= 0; i--) {
        tokens.splice(directiveRanges[i][0], directiveRanges[i][1]);
    }

    return includeFiles;
}

function handleDirectiveTokens(directiveTokens: TokenizerToken[], includeFiles: TokenizerToken[]) {
    directiveTokens[0].highlight.token = HighlightToken.Directive;

    if (directiveTokens[1]?.text === 'include') {
        directiveTokens[1].highlight.token = HighlightToken.Directive;

        // Check the include directive.
        const fileName = directiveTokens[2];
        if (fileName === undefined) {
            diagnostic.addError(directiveTokens[1].location, 'Expected file name for include directive.');
            return;
        }

        if (fileName.kind !== TokenKind.String) {
            diagnostic.addError(directiveTokens[2].location, 'Expected string literal for include directive.');
            return;
        }

        includeFiles.push(fileName);
    } else {
        if (directiveTokens[1] != null) directiveTokens[1].highlight.token = HighlightToken.Label;
    }
}

function sliceTokenListBySameLine(tokens: TokenizerToken[], head: number): TokenizerToken[] {
    let tail = head;
    for (let i = head; i < tokens.length - 1; i++) {
        if (isSameLine(tokens[i].location.end, tokens[i + 1].location.start) === false) {
            break;
        }

        tail = i + 1;
    }

    return tokens.slice(head, tail + 1);
}

function createConnectedStringTokenAt(actualTokens: ParserToken[], index: number): ParserToken {
    const token: Mutable<ParserToken> = createVirtualToken(TokenKind.String, actualTokens[index].text + actualTokens[index + 1].text);
    token.location = {
        path: actualTokens[index].location.path,
        start: actualTokens[index].location.start,
        end: actualTokens[index + 1].location.end,
    };

    return token;
}
